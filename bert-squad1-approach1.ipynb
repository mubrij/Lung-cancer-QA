{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":799923,"sourceType":"datasetVersion","datasetId":374}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%time\n\n# Importing some of the necessary libraries\n\nimport json\nfrom pathlib import Path\nimport time\nimport torch\nfrom torch.utils.data import DataLoader","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-18T11:20:43.706829Z","iopub.execute_input":"2023-09-18T11:20:43.707998Z","iopub.status.idle":"2023-09-18T11:20:47.193786Z","shell.execute_reply.started":"2023-09-18T11:20:43.707943Z","shell.execute_reply":"2023-09-18T11:20:47.191130Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"CPU times: user 1.45 s, sys: 298 ms, total: 1.75 s\nWall time: 3.47 s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%capture\n\n!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:20:47.195958Z","iopub.execute_input":"2023-09-18T11:20:47.196568Z","iopub.status.idle":"2023-09-18T11:21:00.898522Z","shell.execute_reply.started":"2023-09-18T11:20:47.196528Z","shell.execute_reply":"2023-09-18T11:21:00.897203Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Retrieval and Storage of the Data\n\ndef generate_texts_queries_answers(path):\n    with open(path, 'rb') as f:\n        squad_dict = json.load(f)\n    texts, queries, answers = [], [], []\n    for group in squad_dict['data']:\n        for passage in group['paragraphs']:\n            context = passage['context']\n            for qa in passage['qas']:\n                question = qa['question']\n                for answer in qa['answers']:\n                    texts.append(context)\n                    queries.append(question)\n                    answers.append(answer)\n    return texts, queries, answers\n    \n\ntrain_path = Path('../input/stanford-question-answering-dataset/train-v1.1.json')\ntrain_texts, train_queries, train_answers = generate_texts_queries_answers(train_path)\nvalidation_path = Path('../input/stanford-question-answering-dataset/dev-v1.1.json')\nvalidation_texts, validation_queries, validation_answers = generate_texts_queries_answers(validation_path)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:21:00.900820Z","iopub.execute_input":"2023-09-18T11:21:00.901596Z","iopub.status.idle":"2023-09-18T11:21:02.500483Z","shell.execute_reply.started":"2023-09-18T11:21:00.901553Z","shell.execute_reply":"2023-09-18T11:21:02.499450Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"CPU times: user 1.01 s, sys: 156 ms, total: 1.17 s\nWall time: 1.59 s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n\n# Checking the lengths of the 6 lists obtained above\n\nprint (len(train_texts))\nprint (len(train_queries))\nprint (len(train_answers))\n\nprint (len(validation_texts))\nprint (len(validation_queries))\nprint (len(validation_answers))","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:21:02.503544Z","iopub.execute_input":"2023-09-18T11:21:02.504268Z","iopub.status.idle":"2023-09-18T11:21:02.512339Z","shell.execute_reply.started":"2023-09-18T11:21:02.504230Z","shell.execute_reply":"2023-09-18T11:21:02.511197Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"87599\n87599\n87599\n34726\n34726\n34726\nCPU times: user 796 µs, sys: 0 ns, total: 796 µs\nWall time: 901 µs\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n\n# Checking the 1st entry in each of the 3 train datasets\n\nprint ('Text :\\n', train_texts[0])\nprint ('\\nQuestion :\\n', train_queries[0])\nprint ('\\nAnswer :\\n', train_answers[0])","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:21:02.514444Z","iopub.execute_input":"2023-09-18T11:21:02.515466Z","iopub.status.idle":"2023-09-18T11:21:02.522829Z","shell.execute_reply.started":"2023-09-18T11:21:02.515430Z","shell.execute_reply":"2023-09-18T11:21:02.521771Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Text :\n Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n\nQuestion :\n To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n\nAnswer :\n {'answer_start': 515, 'text': 'Saint Bernadette Soubirous'}\nCPU times: user 0 ns, sys: 662 µs, total: 662 µs\nWall time: 716 µs\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n\n'''\nFixing up end position characters in train and validation data - Processing the SQuAD dataset to keep up with \nthe input that BERT desires!\nBERT Models need both the start and end position characters of the answer and sometimes its been noticed that \nSQuAD answers \"eat up\" 1 or 2 characters from the real answer in the passage\n'''\n\n# Train Data\nfor answer, text in zip(train_answers, train_texts):\n    real_answer = answer['text']\n    answer_start_index = answer['answer_start']\n    answer_end_index = answer_start_index + len(real_answer)\n    if text[answer_start_index : answer_end_index] == real_answer:\n        answer['answer_end'] = answer_end_index\n    elif text[answer_start_index - 1 : answer_end_index - 1] == real_answer:\n        answer['answer_start'] = answer_start_index - 1\n        answer['answer_end'] = answer_end_index - 1\n    elif text[answer_start_index - 2 : answer_end_index - 2] == real_answer:\n        answer['answer_start'] = answer_start_index - 2\n        answer['answer_end'] = answer_end_index - 2\n    \n# Validation Data\nfor answer, text in zip(validation_answers, validation_texts):\n    real_answer = answer['text']\n    answer_start_index = answer['answer_start']\n    answer_end_index = answer_start_index + len(real_answer)\n    if text[answer_start_index : answer_end_index] == real_answer:\n        answer['answer_end'] = answer_end_index\n    elif text[answer_start_index - 1 : answer_end_index - 1] == real_answer:\n        answer['answer_start'] = answer_start_index - 1\n        answer['answer_end'] = answer_end_index - 1\n    elif text[answer_start_index - 2 : answer_end_index - 2] == real_answer:\n        answer['answer_start'] = answer_start_index - 2\n        answer['answer_end'] = answer_end_index - 2    ","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:21:02.524740Z","iopub.execute_input":"2023-09-18T11:21:02.525580Z","iopub.status.idle":"2023-09-18T11:21:02.669231Z","shell.execute_reply.started":"2023-09-18T11:21:02.525545Z","shell.execute_reply":"2023-09-18T11:21:02.667458Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"CPU times: user 126 ms, sys: 388 µs, total: 126 ms\nWall time: 127 ms\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n\n# Tokenization of Passages and Queries\n\nfrom transformers import AutoTokenizer, AdamW, BertForQuestionAnswering\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n\ntrain_encodings = tokenizer(train_texts, train_queries, truncation=True, padding=True)\nvalidation_encodings = tokenizer(validation_texts, validation_queries, truncation=True, padding=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:21:02.670491Z","iopub.execute_input":"2023-09-18T11:21:02.671015Z","iopub.status.idle":"2023-09-18T11:22:24.704153Z","shell.execute_reply.started":"2023-09-18T11:21:02.670979Z","shell.execute_reply":"2023-09-18T11:22:24.702972Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa03822acfce4fcdb7787f96209405a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2846681bcd8444c5bb1144132189a60e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b87e8da90711410499c709db49d77010"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6a9b6a38e44415b83478e5f011a5c02"}},"metadata":{}},{"name":"stdout","text":"CPU times: user 2min 1s, sys: 7.28 s, total: 2min 8s\nWall time: 1min 22s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n\n# Conversion of the start-end positions to the tokens' start-end positions\n\ndef add_token_positions(encodings, answers):\n    start_positions, end_positions = [], []\n    for i in range(len(answers)):\n        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end']))\n        # if start position is None, then it means that the answer passage has been truncated\n        if start_positions[-1] is None:\n            start_positions[-1] = tokenizer.model_max_length\n        # if end position is None, the 'char_to_token' function points to the space after the correct token\n        if end_positions[-1] is None:\n            end_positions[-1] = encodings.char_to_token(i, answers[i]['answer_end'] - 1)\n            # if end position is still None, the answer passage has been truncated\n            if end_positions[-1] is None:\n                end_positions[-1] = tokenizer.model_max_length\n    encodings.update({'start_positions' : start_positions, 'end_positions' : end_positions})\n    \n\nadd_token_positions(train_encodings, train_answers)\nadd_token_positions(validation_encodings, validation_answers)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:22:24.705831Z","iopub.execute_input":"2023-09-18T11:22:24.706207Z","iopub.status.idle":"2023-09-18T11:22:25.186884Z","shell.execute_reply.started":"2023-09-18T11:22:24.706171Z","shell.execute_reply":"2023-09-18T11:22:25.185847Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"CPU times: user 469 ms, sys: 3.96 ms, total: 473 ms\nWall time: 472 ms\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n\n# In order to train and validate previous data more easily and covert encodings to datasets\nclass squadDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n    def __getitem__(self, idx):\n        return {key : torch.tensor(val[idx]) for key, val in self.encodings.items()}\n    def __len__(self):\n        return len(self.encodings.input_ids)\n        \n\ntrain_dataset = squadDataset(train_encodings)\nvalidation_dataset = squadDataset(validation_encodings)\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nvalidation_loader = DataLoader(validation_dataset, batch_size=8, shuffle=True)\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:22:25.188252Z","iopub.execute_input":"2023-09-18T11:22:25.189428Z","iopub.status.idle":"2023-09-18T11:22:25.291032Z","shell.execute_reply.started":"2023-09-18T11:22:25.189390Z","shell.execute_reply":"2023-09-18T11:22:25.288987Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"CPU times: user 7.41 ms, sys: 54.2 ms, total: 61.6 ms\nWall time: 91.8 ms\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n\n'''\nBuilding the BERT Model - when a model is instantiated with 'from_pretrained()', the model configuration and \npre-trained weights of the specified model are used to initialize the model\nAdamW is used as the PyTorch optimizer here, since it implements gradient bias correction as well as weight decay.\n'''\nmodel = BertForQuestionAnswering.from_pretrained('bert-base-uncased').to(device)\noptimizer = AdamW(model.parameters(), lr=5e-5)\nepochs = 3","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:22:25.295027Z","iopub.execute_input":"2023-09-18T11:22:25.295606Z","iopub.status.idle":"2023-09-18T11:22:44.399063Z","shell.execute_reply.started":"2023-09-18T11:22:25.295571Z","shell.execute_reply":"2023-09-18T11:22:44.398107Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efd62c376c6d4b51bb543a0092d9f14e"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"CPU times: user 3.71 s, sys: 2.16 s, total: 5.87 s\nWall time: 19.1 s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The above message is a warning that the model should be fine tuned before testing, in order to have a good performance!","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Training and Evaluating the Model\n\ntrain_losses, validation_losses = [], []\nprint_every = 1000\n\n\nfor epoch in range(epochs):\n    epoch_time = time.time()\n    # Set the model in train mode\n    model.train()\n    epoch_loss = 0\n    for batch_idx, batch in enumerate(train_loader):\n        print ('Train', epoch, batch_idx)\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        start_positions = batch['start_positions'].to(device)\n        end_positions = batch['end_positions'].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, \\\n                        end_positions=end_positions)\n        loss = outputs[0]\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n        if (batch_idx + 1) % print_every == 0:\n            print ('Batch {:} / {:}'.format(batch_idx + 1, len(train_loader)), '\\nLoss :', round(loss.item(), 1), '\\n')\n    epoch_loss /= len(train_loader)\n    train_losses.append(epoch_loss)\n    # Set the model in evaluation mode\n    model.eval()\n    epoch_loss = 0\n    for batch_idx, batch in enumerate(validation_loader):\n        print ('Validation', epoch, batch_idx)\n        with torch.no_grad():\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            start_positions = batch['start_positions'].to(device)\n            end_positions = batch['end_positions'].to(device)\n            outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, \\\n                            end_positions=end_positions)\n            loss = outputs[0]\n            epoch_loss += loss.item()\n        if (batch_idx + 1) % print_every == 0:\n            print ('Batch {:} / {:}'.format(batch_idx + 1, len(validation_loader)), '\\nLoss :', round(loss.item(), 1), '\\n')\n    epoch_loss /= len(validation_loader)\n    validation_losses.append(epoch_loss)\n    print ('\\n---Epoch', epoch + 1, '---', '\\nTraining Loss :', train_losses[-1], '---', '\\nValidation Loss :', \\\n           validation_losses[-1], '\\n---\\n')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:23:10.344941Z","iopub.execute_input":"2023-09-18T11:23:10.345323Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Train 0 0\nTrain 0 1\nTrain 0 2\nTrain 0 3\nTrain 0 4\nTrain 0 5\nTrain 0 6\nTrain 0 7\nTrain 0 8\nTrain 0 9\nTrain 0 10\nTrain 0 11\nTrain 0 12\nTrain 0 13\nTrain 0 14\nTrain 0 15\nTrain 0 16\nTrain 0 17\nTrain 0 18\nTrain 0 19\nTrain 0 20\nTrain 0 21\nTrain 0 22\nTrain 0 23\nTrain 0 24\nTrain 0 25\nTrain 0 26\nTrain 0 27\nTrain 0 28\nTrain 0 29\nTrain 0 30\nTrain 0 31\nTrain 0 32\nTrain 0 33\nTrain 0 34\nTrain 0 35\nTrain 0 36\nTrain 0 37\nTrain 0 38\nTrain 0 39\nTrain 0 40\nTrain 0 41\nTrain 0 42\nTrain 0 43\nTrain 0 44\nTrain 0 45\nTrain 0 46\nTrain 0 47\nTrain 0 48\nTrain 0 49\nTrain 0 50\nTrain 0 51\nTrain 0 52\nTrain 0 53\nTrain 0 54\nTrain 0 55\nTrain 0 56\nTrain 0 57\nTrain 0 58\nTrain 0 59\nTrain 0 60\nTrain 0 61\nTrain 0 62\nTrain 0 63\nTrain 0 64\nTrain 0 65\nTrain 0 66\nTrain 0 67\nTrain 0 68\nTrain 0 69\nTrain 0 70\nTrain 0 71\nTrain 0 72\nTrain 0 73\nTrain 0 74\nTrain 0 75\nTrain 0 76\nTrain 0 77\nTrain 0 78\nTrain 0 79\nTrain 0 80\nTrain 0 81\nTrain 0 82\nTrain 0 83\nTrain 0 84\nTrain 0 85\nTrain 0 86\nTrain 0 87\nTrain 0 88\nTrain 0 89\nTrain 0 90\nTrain 0 91\nTrain 0 92\nTrain 0 93\nTrain 0 94\nTrain 0 95\nTrain 0 96\nTrain 0 97\nTrain 0 98\nTrain 0 99\nTrain 0 100\nTrain 0 101\nTrain 0 102\nTrain 0 103\nTrain 0 104\nTrain 0 105\nTrain 0 106\nTrain 0 107\nTrain 0 108\nTrain 0 109\nTrain 0 110\nTrain 0 111\nTrain 0 112\nTrain 0 113\nTrain 0 114\nTrain 0 115\nTrain 0 116\nTrain 0 117\nTrain 0 118\nTrain 0 119\nTrain 0 120\nTrain 0 121\nTrain 0 122\nTrain 0 123\nTrain 0 124\nTrain 0 125\nTrain 0 126\nTrain 0 127\nTrain 0 128\nTrain 0 129\nTrain 0 130\nTrain 0 131\nTrain 0 132\nTrain 0 133\nTrain 0 134\nTrain 0 135\nTrain 0 136\nTrain 0 137\nTrain 0 138\nTrain 0 139\nTrain 0 140\nTrain 0 141\nTrain 0 142\nTrain 0 143\nTrain 0 144\nTrain 0 145\nTrain 0 146\nTrain 0 147\nTrain 0 148\nTrain 0 149\nTrain 0 150\nTrain 0 151\nTrain 0 152\nTrain 0 153\nTrain 0 154\nTrain 0 155\nTrain 0 156\nTrain 0 157\nTrain 0 158\nTrain 0 159\nTrain 0 160\nTrain 0 161\nTrain 0 162\nTrain 0 163\nTrain 0 164\nTrain 0 165\nTrain 0 166\nTrain 0 167\nTrain 0 168\nTrain 0 169\nTrain 0 170\nTrain 0 171\nTrain 0 172\nTrain 0 173\nTrain 0 174\nTrain 0 175\nTrain 0 176\nTrain 0 177\nTrain 0 178\nTrain 0 179\nTrain 0 180\nTrain 0 181\nTrain 0 182\nTrain 0 183\nTrain 0 184\nTrain 0 185\nTrain 0 186\nTrain 0 187\nTrain 0 188\nTrain 0 189\nTrain 0 190\nTrain 0 191\nTrain 0 192\nTrain 0 193\nTrain 0 194\nTrain 0 195\nTrain 0 196\nTrain 0 197\nTrain 0 198\nTrain 0 199\nTrain 0 200\nTrain 0 201\nTrain 0 202\nTrain 0 203\nTrain 0 204\nTrain 0 205\nTrain 0 206\nTrain 0 207\nTrain 0 208\nTrain 0 209\nTrain 0 210\nTrain 0 211\nTrain 0 212\nTrain 0 213\nTrain 0 214\nTrain 0 215\nTrain 0 216\nTrain 0 217\nTrain 0 218\nTrain 0 219\nTrain 0 220\nTrain 0 221\nTrain 0 222\nTrain 0 223\nTrain 0 224\nTrain 0 225\nTrain 0 226\nTrain 0 227\nTrain 0 228\nTrain 0 229\nTrain 0 230\nTrain 0 231\nTrain 0 232\nTrain 0 233\nTrain 0 234\nTrain 0 235\nTrain 0 236\nTrain 0 237\nTrain 0 238\nTrain 0 239\nTrain 0 240\nTrain 0 241\nTrain 0 242\nTrain 0 243\nTrain 0 244\nTrain 0 245\nTrain 0 246\nTrain 0 247\nTrain 0 248\nTrain 0 249\nTrain 0 250\nTrain 0 251\nTrain 0 252\nTrain 0 253\nTrain 0 254\nTrain 0 255\nTrain 0 256\nTrain 0 257\nTrain 0 258\nTrain 0 259\nTrain 0 260\nTrain 0 261\nTrain 0 262\nTrain 0 263\nTrain 0 264\nTrain 0 265\nTrain 0 266\nTrain 0 267\nTrain 0 268\nTrain 0 269\nTrain 0 270\nTrain 0 271\nTrain 0 272\nTrain 0 273\nTrain 0 274\nTrain 0 275\nTrain 0 276\nTrain 0 277\nTrain 0 278\nTrain 0 279\nTrain 0 280\nTrain 0 281\nTrain 0 282\nTrain 0 283\nTrain 0 284\nTrain 0 285\nTrain 0 286\nTrain 0 287\nTrain 0 288\nTrain 0 289\nTrain 0 290\nTrain 0 291\nTrain 0 292\nTrain 0 293\nTrain 0 294\nTrain 0 295\nTrain 0 296\nTrain 0 297\nTrain 0 298\nTrain 0 299\nTrain 0 300\nTrain 0 301\nTrain 0 302\nTrain 0 303\nTrain 0 304\nTrain 0 305\nTrain 0 306\nTrain 0 307\nTrain 0 308\nTrain 0 309\nTrain 0 310\nTrain 0 311\nTrain 0 312\nTrain 0 313\nTrain 0 314\nTrain 0 315\nTrain 0 316\nTrain 0 317\nTrain 0 318\nTrain 0 319\nTrain 0 320\nTrain 0 321\nTrain 0 322\nTrain 0 323\nTrain 0 324\nTrain 0 325\nTrain 0 326\nTrain 0 327\nTrain 0 328\nTrain 0 329\nTrain 0 330\nTrain 0 331\nTrain 0 332\nTrain 0 333\nTrain 0 334\nTrain 0 335\nTrain 0 336\nTrain 0 337\nTrain 0 338\nTrain 0 339\nTrain 0 340\nTrain 0 341\nTrain 0 342\nTrain 0 343\nTrain 0 344\nTrain 0 345\nTrain 0 346\nTrain 0 347\nTrain 0 348\nTrain 0 349\nTrain 0 350\nTrain 0 351\nTrain 0 352\nTrain 0 353\nTrain 0 354\nTrain 0 355\nTrain 0 356\nTrain 0 357\nTrain 0 358\nTrain 0 359\nTrain 0 360\nTrain 0 361\nTrain 0 362\nTrain 0 363\nTrain 0 364\nTrain 0 365\nTrain 0 366\nTrain 0 367\nTrain 0 368\nTrain 0 369\nTrain 0 370\nTrain 0 371\nTrain 0 372\nTrain 0 373\nTrain 0 374\nTrain 0 375\nTrain 0 376\nTrain 0 377\nTrain 0 378\nTrain 0 379\nTrain 0 380\nTrain 0 381\nTrain 0 382\nTrain 0 383\nTrain 0 384\nTrain 0 385\nTrain 0 386\nTrain 0 387\nTrain 0 388\nTrain 0 389\nTrain 0 390\nTrain 0 391\nTrain 0 392\nTrain 0 393\nTrain 0 394\nTrain 0 395\nTrain 0 396\nTrain 0 397\nTrain 0 398\nTrain 0 399\nTrain 0 400\nTrain 0 401\nTrain 0 402\nTrain 0 403\nTrain 0 404\nTrain 0 405\nTrain 0 406\nTrain 0 407\nTrain 0 408\nTrain 0 409\nTrain 0 410\nTrain 0 411\nTrain 0 412\nTrain 0 413\nTrain 0 414\nTrain 0 415\nTrain 0 416\nTrain 0 417\nTrain 0 418\nTrain 0 419\nTrain 0 420\nTrain 0 421\nTrain 0 422\nTrain 0 423\nTrain 0 424\nTrain 0 425\nTrain 0 426\nTrain 0 427\nTrain 0 428\nTrain 0 429\nTrain 0 430\nTrain 0 431\nTrain 0 432\nTrain 0 433\nTrain 0 434\nTrain 0 435\nTrain 0 436\nTrain 0 437\nTrain 0 438\nTrain 0 439\nTrain 0 440\nTrain 0 441\nTrain 0 442\nTrain 0 443\nTrain 0 444\nTrain 0 445\nTrain 0 446\nTrain 0 447\nTrain 0 448\nTrain 0 449\nTrain 0 450\nTrain 0 451\nTrain 0 452\nTrain 0 453\nTrain 0 454\nTrain 0 455\nTrain 0 456\nTrain 0 457\nTrain 0 458\nTrain 0 459\nTrain 0 460\nTrain 0 461\nTrain 0 462\nTrain 0 463\nTrain 0 464\nTrain 0 465\nTrain 0 466\nTrain 0 467\nTrain 0 468\nTrain 0 469\nTrain 0 470\nTrain 0 471\nTrain 0 472\nTrain 0 473\nTrain 0 474\nTrain 0 475\nTrain 0 476\nTrain 0 477\nTrain 0 478\nTrain 0 479\nTrain 0 480\nTrain 0 481\nTrain 0 482\nTrain 0 483\nTrain 0 484\nTrain 0 485\nTrain 0 486\nTrain 0 487\nTrain 0 488\nTrain 0 489\nTrain 0 490\nTrain 0 491\nTrain 0 492\nTrain 0 493\nTrain 0 494\nTrain 0 495\nTrain 0 496\nTrain 0 497\nTrain 0 498\nTrain 0 499\nTrain 0 500\nTrain 0 501\nTrain 0 502\nTrain 0 503\nTrain 0 504\nTrain 0 505\nTrain 0 506\nTrain 0 507\nTrain 0 508\nTrain 0 509\nTrain 0 510\nTrain 0 511\nTrain 0 512\nTrain 0 513\nTrain 0 514\nTrain 0 515\nTrain 0 516\nTrain 0 517\nTrain 0 518\nTrain 0 519\nTrain 0 520\nTrain 0 521\nTrain 0 522\nTrain 0 523\nTrain 0 524\nTrain 0 525\nTrain 0 526\nTrain 0 527\nTrain 0 528\nTrain 0 529\nTrain 0 530\nTrain 0 531\nTrain 0 532\nTrain 0 533\nTrain 0 534\nTrain 0 535\nTrain 0 536\nTrain 0 537\nTrain 0 538\nTrain 0 539\nTrain 0 540\nTrain 0 541\nTrain 0 542\nTrain 0 543\nTrain 0 544\nTrain 0 545\nTrain 0 546\nTrain 0 547\nTrain 0 548\nTrain 0 549\nTrain 0 550\nTrain 0 551\nTrain 0 552\nTrain 0 553\nTrain 0 554\nTrain 0 555\nTrain 0 556\nTrain 0 557\nTrain 0 558\nTrain 0 559\nTrain 0 560\nTrain 0 561\nTrain 0 562\nTrain 0 563\nTrain 0 564\nTrain 0 565\nTrain 0 566\nTrain 0 567\nTrain 0 568\nTrain 0 569\nTrain 0 570\nTrain 0 571\nTrain 0 572\nTrain 0 573\nTrain 0 574\nTrain 0 575\nTrain 0 576\nTrain 0 577\nTrain 0 578\nTrain 0 579\nTrain 0 580\nTrain 0 581\nTrain 0 582\nTrain 0 583\nTrain 0 584\nTrain 0 585\nTrain 0 586\nTrain 0 587\nTrain 0 588\nTrain 0 589\nTrain 0 590\nTrain 0 591\nTrain 0 592\nTrain 0 593\nTrain 0 594\nTrain 0 595\nTrain 0 596\nTrain 0 597\nTrain 0 598\nTrain 0 599\nTrain 0 600\nTrain 0 601\nTrain 0 602\nTrain 0 603\nTrain 0 604\nTrain 0 605\nTrain 0 606\nTrain 0 607\nTrain 0 608\nTrain 0 609\nTrain 0 610\nTrain 0 611\nTrain 0 612\nTrain 0 613\nTrain 0 614\nTrain 0 615\nTrain 0 616\nTrain 0 617\nTrain 0 618\nTrain 0 619\nTrain 0 620\nTrain 0 621\nTrain 0 622\nTrain 0 623\nTrain 0 624\nTrain 0 625\nTrain 0 626\nTrain 0 627\nTrain 0 628\nTrain 0 629\nTrain 0 630\nTrain 0 631\nTrain 0 632\nTrain 0 633\nTrain 0 634\nTrain 0 635\nTrain 0 636\nTrain 0 637\nTrain 0 638\nTrain 0 639\nTrain 0 640\nTrain 0 641\nTrain 0 642\nTrain 0 643\nTrain 0 644\nTrain 0 645\nTrain 0 646\nTrain 0 647\nTrain 0 648\nTrain 0 649\nTrain 0 650\nTrain 0 651\nTrain 0 652\nTrain 0 653\nTrain 0 654\nTrain 0 655\nTrain 0 656\nTrain 0 657\nTrain 0 658\nTrain 0 659\nTrain 0 660\nTrain 0 661\nTrain 0 662\nTrain 0 663\nTrain 0 664\nTrain 0 665\nTrain 0 666\nTrain 0 667\nTrain 0 668\nTrain 0 669\nTrain 0 670\nTrain 0 671\nTrain 0 672\nTrain 0 673\nTrain 0 674\nTrain 0 675\nTrain 0 676\nTrain 0 677\nTrain 0 678\nTrain 0 679\nTrain 0 680\nTrain 0 681\nTrain 0 682\nTrain 0 683\nTrain 0 684\nTrain 0 685\nTrain 0 686\nTrain 0 687\nTrain 0 688\nTrain 0 689\nTrain 0 690\nTrain 0 691\nTrain 0 692\nTrain 0 693\nTrain 0 694\nTrain 0 695\nTrain 0 696\nTrain 0 697\nTrain 0 698\nTrain 0 699\nTrain 0 700\nTrain 0 701\nTrain 0 702\nTrain 0 703\nTrain 0 704\nTrain 0 705\nTrain 0 706\nTrain 0 707\nTrain 0 708\nTrain 0 709\nTrain 0 710\nTrain 0 711\nTrain 0 712\nTrain 0 713\nTrain 0 714\nTrain 0 715\nTrain 0 716\nTrain 0 717\nTrain 0 718\nTrain 0 719\nTrain 0 720\nTrain 0 721\nTrain 0 722\nTrain 0 723\nTrain 0 724\nTrain 0 725\nTrain 0 726\nTrain 0 727\nTrain 0 728\nTrain 0 729\nTrain 0 730\nTrain 0 731\nTrain 0 732\nTrain 0 733\nTrain 0 734\nTrain 0 735\nTrain 0 736\nTrain 0 737\nTrain 0 738\nTrain 0 739\nTrain 0 740\nTrain 0 741\nTrain 0 742\nTrain 0 743\nTrain 0 744\nTrain 0 745\nTrain 0 746\nTrain 0 747\nTrain 0 748\nTrain 0 749\nTrain 0 750\nTrain 0 751\nTrain 0 752\nTrain 0 753\nTrain 0 754\nTrain 0 755\nTrain 0 756\nTrain 0 757\nTrain 0 758\nTrain 0 759\nTrain 0 760\nTrain 0 761\nTrain 0 762\nTrain 0 763\nTrain 0 764\nTrain 0 765\nTrain 0 766\nTrain 0 767\nTrain 0 768\nTrain 0 769\nTrain 0 770\nTrain 0 771\nTrain 0 772\nTrain 0 773\nTrain 0 774\nTrain 0 775\nTrain 0 776\nTrain 0 777\nTrain 0 778\nTrain 0 779\nTrain 0 780\nTrain 0 781\nTrain 0 782\nTrain 0 783\nTrain 0 784\nTrain 0 785\nTrain 0 786\nTrain 0 787\nTrain 0 788\nTrain 0 789\nTrain 0 790\nTrain 0 791\nTrain 0 792\nTrain 0 793\nTrain 0 794\nTrain 0 795\nTrain 0 796\nTrain 0 797\nTrain 0 798\nTrain 0 799\nTrain 0 800\nTrain 0 801\nTrain 0 802\nTrain 0 803\nTrain 0 804\nTrain 0 805\nTrain 0 806\nTrain 0 807\nTrain 0 808\nTrain 0 809\nTrain 0 810\nTrain 0 811\nTrain 0 812\nTrain 0 813\nTrain 0 814\nTrain 0 815\nTrain 0 816\nTrain 0 817\nTrain 0 818\nTrain 0 819\nTrain 0 820\nTrain 0 821\nTrain 0 822\nTrain 0 823\nTrain 0 824\nTrain 0 825\nTrain 0 826\nTrain 0 827\nTrain 0 828\nTrain 0 829\nTrain 0 830\nTrain 0 831\nTrain 0 832\nTrain 0 833\nTrain 0 834\nTrain 0 835\nTrain 0 836\nTrain 0 837\nTrain 0 838\nTrain 0 839\nTrain 0 840\nTrain 0 841\nTrain 0 842\nTrain 0 843\nTrain 0 844\nTrain 0 845\nTrain 0 846\nTrain 0 847\nTrain 0 848\nTrain 0 849\nTrain 0 850\nTrain 0 851\nTrain 0 852\nTrain 0 853\nTrain 0 854\nTrain 0 855\nTrain 0 856\nTrain 0 857\nTrain 0 858\nTrain 0 859\nTrain 0 860\nTrain 0 861\nTrain 0 862\nTrain 0 863\nTrain 0 864\nTrain 0 865\nTrain 0 866\nTrain 0 867\nTrain 0 868\nTrain 0 869\nTrain 0 870\nTrain 0 871\nTrain 0 872\nTrain 0 873\nTrain 0 874\nTrain 0 875\nTrain 0 876\nTrain 0 877\nTrain 0 878\nTrain 0 879\nTrain 0 880\nTrain 0 881\nTrain 0 882\nTrain 0 883\nTrain 0 884\nTrain 0 885\nTrain 0 886\nTrain 0 887\nTrain 0 888\nTrain 0 889\nTrain 0 890\nTrain 0 891\nTrain 0 892\nTrain 0 893\nTrain 0 894\nTrain 0 895\nTrain 0 896\nTrain 0 897\nTrain 0 898\nTrain 0 899\nTrain 0 900\nTrain 0 901\nTrain 0 902\nTrain 0 903\nTrain 0 904\nTrain 0 905\nTrain 0 906\nTrain 0 907\nTrain 0 908\nTrain 0 909\nTrain 0 910\nTrain 0 911\nTrain 0 912\nTrain 0 913\nTrain 0 914\nTrain 0 915\nTrain 0 916\nTrain 0 917\nTrain 0 918\nTrain 0 919\nTrain 0 920\nTrain 0 921\nTrain 0 922\nTrain 0 923\nTrain 0 924\nTrain 0 925\nTrain 0 926\nTrain 0 927\nTrain 0 928\nTrain 0 929\nTrain 0 930\nTrain 0 931\nTrain 0 932\nTrain 0 933\nTrain 0 934\nTrain 0 935\nTrain 0 936\nTrain 0 937\nTrain 0 938\nTrain 0 939\nTrain 0 940\nTrain 0 941\nTrain 0 942\nTrain 0 943\nTrain 0 944\nTrain 0 945\nTrain 0 946\nTrain 0 947\nTrain 0 948\nTrain 0 949\nTrain 0 950\nTrain 0 951\nTrain 0 952\nTrain 0 953\nTrain 0 954\nTrain 0 955\nTrain 0 956\nTrain 0 957\nTrain 0 958\nTrain 0 959\nTrain 0 960\nTrain 0 961\nTrain 0 962\nTrain 0 963\nTrain 0 964\nTrain 0 965\nTrain 0 966\nTrain 0 967\nTrain 0 968\nTrain 0 969\nTrain 0 970\nTrain 0 971\nTrain 0 972\nTrain 0 973\nTrain 0 974\nTrain 0 975\nTrain 0 976\nTrain 0 977\nTrain 0 978\nTrain 0 979\nTrain 0 980\nTrain 0 981\nTrain 0 982\nTrain 0 983\nTrain 0 984\nTrain 0 985\nTrain 0 986\nTrain 0 987\nTrain 0 988\nTrain 0 989\nTrain 0 990\nTrain 0 991\nTrain 0 992\nTrain 0 993\nTrain 0 994\nTrain 0 995\nTrain 0 996\nTrain 0 997\nTrain 0 998\nTrain 0 999\nBatch 1000 / 10950 \nLoss : 0.6 \n\nTrain 0 1000\nTrain 0 1001\nTrain 0 1002\nTrain 0 1003\nTrain 0 1004\nTrain 0 1005\nTrain 0 1006\nTrain 0 1007\nTrain 0 1008\nTrain 0 1009\nTrain 0 1010\nTrain 0 1011\nTrain 0 1012\nTrain 0 1013\nTrain 0 1014\nTrain 0 1015\nTrain 0 1016\nTrain 0 1017\nTrain 0 1018\nTrain 0 1019\nTrain 0 1020\nTrain 0 1021\nTrain 0 1022\nTrain 0 1023\nTrain 0 1024\nTrain 0 1025\nTrain 0 1026\nTrain 0 1027\nTrain 0 1028\nTrain 0 1029\nTrain 0 1030\nTrain 0 1031\nTrain 0 1032\nTrain 0 1033\nTrain 0 1034\nTrain 0 1035\nTrain 0 1036\nTrain 0 1037\nTrain 0 1038\nTrain 0 1039\nTrain 0 1040\nTrain 0 1041\nTrain 0 1042\nTrain 0 1043\nTrain 0 1044\nTrain 0 1045\nTrain 0 1046\nTrain 0 1047\nTrain 0 1048\nTrain 0 1049\nTrain 0 1050\nTrain 0 1051\nTrain 0 1052\nTrain 0 1053\nTrain 0 1054\nTrain 0 1055\nTrain 0 1056\nTrain 0 1057\nTrain 0 1058\nTrain 0 1059\nTrain 0 1060\nTrain 0 1061\nTrain 0 1062\nTrain 0 1063\nTrain 0 1064\nTrain 0 1065\nTrain 0 1066\nTrain 0 1067\nTrain 0 1068\nTrain 0 1069\nTrain 0 1070\nTrain 0 1071\nTrain 0 1072\nTrain 0 1073\nTrain 0 1074\nTrain 0 1075\nTrain 0 1076\nTrain 0 1077\nTrain 0 1078\nTrain 0 1079\nTrain 0 1080\nTrain 0 1081\nTrain 0 1082\nTrain 0 1083\nTrain 0 1084\nTrain 0 1085\nTrain 0 1086\nTrain 0 1087\nTrain 0 1088\nTrain 0 1089\nTrain 0 1090\nTrain 0 1091\nTrain 0 1092\nTrain 0 1093\nTrain 0 1094\nTrain 0 1095\nTrain 0 1096\nTrain 0 1097\nTrain 0 1098\nTrain 0 1099\nTrain 0 1100\nTrain 0 1101\nTrain 0 1102\nTrain 0 1103\nTrain 0 1104\nTrain 0 1105\nTrain 0 1106\nTrain 0 1107\nTrain 0 1108\nTrain 0 1109\nTrain 0 1110\nTrain 0 1111\nTrain 0 1112\nTrain 0 1113\nTrain 0 1114\nTrain 0 1115\nTrain 0 1116\nTrain 0 1117\nTrain 0 1118\nTrain 0 1119\nTrain 0 1120\nTrain 0 1121\nTrain 0 1122\nTrain 0 1123\nTrain 0 1124\nTrain 0 1125\nTrain 0 1126\nTrain 0 1127\nTrain 0 1128\nTrain 0 1129\nTrain 0 1130\nTrain 0 1131\nTrain 0 1132\nTrain 0 1133\nTrain 0 1134\nTrain 0 1135\nTrain 0 1136\nTrain 0 1137\nTrain 0 1138\nTrain 0 1139\nTrain 0 1140\nTrain 0 1141\nTrain 0 1142\nTrain 0 1143\nTrain 0 1144\nTrain 0 1145\nTrain 0 1146\nTrain 0 1147\nTrain 0 1148\nTrain 0 1149\nTrain 0 1150\nTrain 0 1151\nTrain 0 1152\nTrain 0 1153\nTrain 0 1154\nTrain 0 1155\nTrain 0 1156\nTrain 0 1157\nTrain 0 1158\nTrain 0 1159\nTrain 0 1160\nTrain 0 1161\nTrain 0 1162\nTrain 0 1163\nTrain 0 1164\nTrain 0 1165\nTrain 0 1166\nTrain 0 1167\nTrain 0 1168\nTrain 0 1169\nTrain 0 1170\nTrain 0 1171\nTrain 0 1172\nTrain 0 1173\nTrain 0 1174\nTrain 0 1175\nTrain 0 1176\nTrain 0 1177\nTrain 0 1178\nTrain 0 1179\nTrain 0 1180\nTrain 0 1181\nTrain 0 1182\nTrain 0 1183\nTrain 0 1184\nTrain 0 1185\nTrain 0 1186\nTrain 0 1187\nTrain 0 1188\nTrain 0 1189\nTrain 0 1190\nTrain 0 1191\nTrain 0 1192\nTrain 0 1193\nTrain 0 1194\nTrain 0 1195\nTrain 0 1196\nTrain 0 1197\nTrain 0 1198\nTrain 0 1199\nTrain 0 1200\nTrain 0 1201\nTrain 0 1202\nTrain 0 1203\nTrain 0 1204\nTrain 0 1205\nTrain 0 1206\nTrain 0 1207\nTrain 0 1208\nTrain 0 1209\nTrain 0 1210\nTrain 0 1211\nTrain 0 1212\nTrain 0 1213\nTrain 0 1214\nTrain 0 1215\nTrain 0 1216\nTrain 0 1217\nTrain 0 1218\nTrain 0 1219\nTrain 0 1220\nTrain 0 1221\nTrain 0 1222\nTrain 0 1223\nTrain 0 1224\nTrain 0 1225\nTrain 0 1226\nTrain 0 1227\nTrain 0 1228\nTrain 0 1229\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n\n# Saving the Model\ntorch.save(model, 'bert_model_3_epochs.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nfrom matplotlib import pyplot as plt\n\n# Plotting the train and validation losses\n\nfig, ax = plt.subplots(1, 1, figsize=(15, 10))\n\nax.set_title('Train and Validation Losses', size=20)\nax.set_ylabel('Loss', fontsize=20)\nax.set_xlabel('Epochs', fontsize=25)\n_ = ax.plot(train_losses)\n_ = ax.plot(validation_losses)\n_ = ax.legend(('Train', 'Validation'), loc='upper right')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}